# TODO_2: Switchable Inference Backend Architecture

## üéØ Executive Summary

**Goal:** Enable runtime switching between OpenAI API-based inference and local model inference (llama.cpp, vLLM, Ollama) without code changes.

**Current State:** Hardcoded to OpenAI API only.

**Solution:** Provider abstraction layer with config-driven backend selection.

---

## ‚ö° Quick Reference

**What to implement:**
1. **Provider abstraction**: Common interface for all inference backends
2. **Config-driven switching**: Environment variables control which backend to use
3. **Fallback chain**: API ‚Üí Local ‚Üí Error (configurable)
4. **Health checks**: Backend availability detection
5. **Container support**: Docker services for local inference backends

**Supported backends:**
- ‚úÖ OpenAI API (existing)
- üÜï Ollama (easiest local deployment)
- üÜï **vLLM (RECOMMENDED - high-performance local serving)**
- üÜï llama.cpp (lightweight, CPU-friendly)
- üÜï Text Generation WebUI (feature-rich local UI)

**Why vLLM for production:**
- ‚ö° **Highest throughput**: PagedAttention, continuous batching
- üéØ **OpenAI-compatible**: Drop-in replacement for OpenAI API
- üìä **Production-ready**: Used by major companies at scale
- üîß **Tensor parallelism**: Multi-GPU support out of the box
- üíæ **Memory efficient**: Serves larger models with less VRAM

**Benefits:**
- üí∞ Cost control: Switch to local for high-volume/development
- üîí Privacy: Sensitive data never leaves infrastructure
- üöÄ Performance: Reduce latency with local models
- üõ°Ô∏è Resilience: Fallback if API unavailable
- üß™ Testing: Deterministic local models for CI/CD

---

## üèóÔ∏è Architecture Design

### Provider Abstraction Pattern

```
User Request
    ‚Üì
FastAPI Endpoint
    ‚Üì
InferenceRouter (config-driven)
    ‚Üì
    ‚îú‚îÄ‚Üí OpenAIProvider (external API)
    ‚îú‚îÄ‚Üí OllamaProvider (local HTTP)
    ‚îú‚îÄ‚Üí vLLMProvider (local HTTP)
    ‚îú‚îÄ‚Üí LlamaCppProvider (local HTTP)
    ‚îî‚îÄ‚Üí TextGenProvider (local HTTP)
    ‚Üì
Standardized Response
```

### Configuration Strategy

**Environment Variables:**
```bash
# Primary backend (required)
INFERENCE_PROVIDER=openai  # openai|ollama|vllm|llamacpp|textgen

# OpenAI API config (if INFERENCE_PROVIDER=openai)
OPENAI_API_KEY=sk-...
OPENAI_API_BASE=https://api.openai.com/v1

# Local inference config (if INFERENCE_PROVIDER=ollama|vllm|etc)
LOCAL_INFERENCE_URL=http://ollama:11434
LOCAL_MODEL_NAME=llama3.1:8b

# Fallback behavior
ENABLE_FALLBACK=true
FALLBACK_CHAIN=openai,ollama,vllm  # Try in order

# Health check
HEALTH_CHECK_INTERVAL=60  # seconds
HEALTH_CHECK_TIMEOUT=5
```

### Provider Interface

All providers implement common interface:

```python
class InferenceProvider(ABC):
    @abstractmethod
    def create_chat_completion(
        self,
        messages: list[dict],
        model: str,
        temperature: float = 0.7,
        max_tokens: int | None = None,
        **kwargs
    ) -> dict:
        """Return OpenAI-compatible response format."""
        pass
    
    @abstractmethod
    def create_embedding(
        self,
        text: str | list[str],
        model: str = "default",
        **kwargs
    ) -> dict:
        """Return OpenAI-compatible embedding response."""
        pass
    
    @abstractmethod
    def is_healthy(self) -> bool:
        """Check if backend is available."""
        pass
    
    @abstractmethod
    def get_available_models(self) -> list[str]:
        """List models available from this provider."""
        pass
```

---

## üê≥ Docker Compose Strategy

### Option 1: Separate Services (Recommended)

**Pros:**
- Independent scaling
- Mix and match providers
- Easy to add/remove backends
- Better resource isolation

**Cons:**
- More containers to manage
- Slightly more complex networking

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  # Existing API service
  aletheia-api:
    build: .
    environment:
      - INFERENCE_PROVIDER=${INFERENCE_PROVIDER:-openai}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LOCAL_INFERENCE_URL=${LOCAL_INFERENCE_URL}
      - ENABLE_FALLBACK=${ENABLE_FALLBACK:-false}
    networks:
      - aletheia-network
    depends_on:
      - postgres
      # Conditionally depend on local inference (via profiles)

  # Ollama backend (lightweight, easy setup)
  ollama:
    image: ollama/ollama:latest
    profiles: ["local-inference", "ollama"]
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    networks:
      - aletheia-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # vLLM backend (high-performance, RECOMMENDED for production)
  # This config is optimized for Llama 3.1 70B on RTX 5090 + RTX 4090
  vllm:
    image: vllm/vllm-openai:latest
    profiles: ["local-inference", "vllm"]
    ports:
      - "8001:8000"
    environment:
      # ===== LLAMA 3.1 70B CONFIGURATION =====
      # Target: RTX 5090 FE (32GB) + RTX 4090 (24GB) = 56GB total
      
      # Model configuration
      - MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct  # Production target
      # For testing: meta-llama/Llama-3.1-8B-Instruct
      
      # Multi-GPU tensor parallelism (CRITICAL for 70B)
      - TENSOR_PARALLEL_SIZE=2  # Split across both GPUs (5090 + 4090)
      - PIPELINE_PARALLEL_SIZE=1  # Keep at 1
      
      # Memory management (AGGRESSIVE for 70B on 56GB)
      - GPU_MEMORY_UTILIZATION=0.95  # Use 95% of VRAM (~53GB usable)
      # Start at 0.92 if OOM, increase to 0.95 if stable
      
      # Context window (TUNED for available VRAM)
      - MAX_MODEL_LEN=4096  # Conservative for 70B (can try 6144 or 8192 if stable)
      # Llama 3.1 supports 128k context, but longer = more VRAM
      
      # Throughput tuning (REDUCED for large model)
      - MAX_NUM_SEQS=32  # Lower for 70B (was 256 for 8B models)
      - MAX_NUM_BATCHED_TOKENS=4096  # Match MAX_MODEL_LEN
      
      # Data type
      - DTYPE=float16  # FP16 half-precision as requested
      # bfloat16 also works on Ampere+ GPUs (5090, 4090)
      
      # Advanced memory optimization
      - ENABLE_CHUNKED_PREFILL=true  # Helps with memory efficiency
      
      # HuggingFace token (REQUIRED for Llama gated models)
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      
      # GPU selection (both GPUs visible)
      - CUDA_VISIBLE_DEVICES=0,1
      
      # Optional: Quantization (backup if OOM)
      # - QUANTIZATION=awq  # Reduces to ~40GB (quality tradeoff)
    
    volumes:
      - vllm-models:/root/.cache/huggingface
    
    networks:
      - aletheia-network
    
    # Resource limits (IMPORTANT - expose both GPUs)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2  # BOTH GPUs: RTX 5090 + RTX 4090
              capabilities: [gpu]
        limits:
          # System resources for 70B model
          cpus: '16'  # Use plenty of CPU cores for preprocessing
          memory: 64G  # Plenty of headroom (you have 128GB)
    
    # Health check (LONG start period for 70B model load)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s  # 5 minutes for 70B model loading (2-4 min typical)
    
    # Restart policy
    restart: unless-stopped

  # llama.cpp backend (CPU-friendly)
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    profiles: ["local-inference", "llamacpp"]
    ports:
      - "8002:8080"
    command: 
      - "--model"
      - "/models/llama-3.1-8b.Q4_K_M.gguf"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
    volumes:
      - llamacpp-models:/models
    networks:
      - aletheia-network

  # Text Generation WebUI (feature-rich)
  textgen:
    image: ghcr.io/oobabooga/text-generation-webui:latest
    profiles: ["local-inference", "textgen"]
    ports:
      - "7860:7860"  # UI
      - "5000:5000"  # API
    environment:
      - CLI_ARGS=--api --listen
    volumes:
      - textgen-models:/app/models
      - textgen-loras:/app/loras
    networks:
      - aletheia-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Postgres (existing)
  postgres:
    image: pgvector/pgvector:pg15
    # ... existing config ...

volumes:
  ollama-models:
  vllm-models:
  llamacpp-models:
  textgen-models:
  textgen-loras:

networks:
  aletheia-network:
    driver: bridge
```

**Usage:**
```bash
# OpenAI API only (default)
docker compose up aletheia-api postgres

# With Ollama local inference
docker compose --profile ollama up

# With vLLM local inference
docker compose --profile vllm up

# All local inference backends (dev/testing)
docker compose --profile local-inference up

# Specific combination
docker compose --profile ollama --profile vllm up
```

### Option 2: Sidecar Pattern

**Pros:**
- Tighter coupling
- Simpler networking (localhost)
- Guaranteed co-location

**Cons:**
- Can't scale independently
- Must restart both if one needs updating
- Higher resource overhead

**Implementation:**
```yaml
services:
  aletheia-api:
    build: .
    environment:
      - INFERENCE_PROVIDER=ollama
      - LOCAL_INFERENCE_URL=http://localhost:11434
    network_mode: "service:ollama"  # Share network namespace
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    ports:
      - "8000:8000"  # API port (shared namespace)
      - "11434:11434"  # Ollama port
```

### Option 3: All-in-One Container (Not Recommended)

**Pros:**
- Single container
- Simplest deployment

**Cons:**
- ‚ö†Ô∏è Violates single-responsibility principle
- ‚ö†Ô∏è Huge image size (FastAPI + PyTorch + models)
- ‚ö†Ô∏è Resource contention
- ‚ö†Ô∏è Difficult to debug/monitor
- ‚ö†Ô∏è Can't use different providers without rebuilding

**Recommendation:** ‚ùå Avoid this approach

---

## ÔøΩ vLLM Production Considerations (IMPORTANT)

### Why vLLM is Recommended

vLLM is the **best choice for production local inference** because:

1. **Performance**: 10-24x higher throughput than vanilla HuggingFace
2. **Efficiency**: PagedAttention reduces memory waste by 50%+
3. **Compatibility**: Drop-in replacement for OpenAI API
4. **Production-ready**: Used by Anthropic, Meta, and others at scale
5. **Active development**: Rapid bug fixes and feature additions

### Hardware Requirements

**Minimum (8B models like Llama-3.1-8B):**
- GPU: 1x NVIDIA GPU with 16GB+ VRAM (RTX 3090, A4000, A5000, L4)
- CPU: 8+ cores
- RAM: 32GB
- Storage: 50GB SSD (for model cache)

**Recommended (8B models, production):**
- GPU: 1x NVIDIA GPU with 24GB+ VRAM (RTX 4090, A5000, A6000, L40)
- CPU: 16+ cores
- RAM: 64GB
- Storage: 100GB NVMe SSD

**For larger models (70B):**
- GPU: 2-4x NVIDIA A100 (80GB) or H100
- CPU: 32+ cores
- RAM: 128GB+
- Storage: 200GB+ NVMe SSD

**Your logos server specs (CONFIRMED):**
- GPU: 1x RTX 5090 FE (32GB VRAM) + 1x RTX 4090 (24GB VRAM) = **56GB total VRAM**
- RAM: 128GB system RAM
- Status: ‚úÖ **Perfect for Llama 3.1 70B FP16 with tensor parallelism!**

### Model Selection Guide

| Model | Size | VRAM Required | Use Case | logos Fit |
|-------|------|---------------|----------|-----------|
| **Llama-3.1-8B-Instruct** | 8B | 16GB (FP16) | Development/testing | ‚úÖ Easy (single GPU) |
| Llama-3.1-8B (AWQ 4-bit) | 8B | 6GB | CPU-heavy systems | ‚úÖ Overkill for your setup |
| **Llama-3.1-70B-Instruct** | 70B | ~140GB (FP16) | **YOUR TARGET** | ‚ö†Ô∏è See multi-GPU config |
| Llama-3.1-70B (FP16, TP=2) | 70B | **~52-56GB (split)** | **Multi-GPU optimized** | ‚úÖ‚úÖ **PERFECT FIT!** |
| Llama-3.1-70B (AWQ 4-bit) | 70B | 40GB | Balance quality/VRAM | ‚úÖ Backup option |
| Mistral-7B-Instruct-v0.3 | 7B | 14GB | Fast, efficient | ‚úÖ Easy |
| Mixtral-8x7B-Instruct | 47B | 90GB | High quality, MoE | ‚ö†Ô∏è Tight fit |

**Recommendation for logos:**
1. **Start with:** Llama-3.1-8B-Instruct (validate setup, ~5 min)
2. **Production target:** Llama-3.1-70B-Instruct with tensor parallelism (TP=2)
3. **Backup option:** If OOM, use AWQ 4-bit quantized 70B (~40GB)

### üéØ Llama 3.1 70B FP16 Configuration (YOUR SETUP)

**VRAM Calculation:**
```
Llama-3.1-70B-Instruct FP16:
- Model weights: ~140GB (70B params √ó 2 bytes/param)
- With TP=2: Split across 2 GPUs = ~70GB per GPU
- KV cache + activations: ~10-15GB per GPU
- Total per GPU: ~80-85GB theoretical

Your hardware:
- RTX 5090 FE: 32GB VRAM
- RTX 4090: 24GB VRAM
- Total: 56GB VRAM

‚ùå 56GB < 140GB full model
‚úÖ But vLLM's tensor parallelism + memory optimization makes it work!

Real-world with GPU_MEMORY_UTILIZATION=0.95:
- RTX 5090: 30.4GB usable
- RTX 4090: 22.8GB usable  
- Total: 53.2GB usable

With vLLM optimizations (PagedAttention, KV cache sharing):
- Estimated fit: ~48-52GB for 70B model + KV cache
- Verdict: ‚úÖ SHOULD FIT with tight tuning!
```

**Optimal vLLM Settings for Your Hardware:**

```bash
# ===================================
# Llama 3.1 70B on 5090 + 4090 Setup
# ===================================

# Model (CRITICAL)
VLLM_MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct

# Tensor parallelism: SPLIT ACROSS BOTH GPUs
VLLM_TENSOR_PARALLEL_SIZE=2  # 5090 + 4090 = 2 GPUs
VLLM_PIPELINE_PARALLEL_SIZE=1  # Keep at 1 for TP

# Memory management (AGGRESSIVE tuning for 70B)
VLLM_GPU_MEMORY_UTILIZATION=0.95  # Use 95% of VRAM (53GB usable)

# Context window (REDUCE to fit in VRAM)
VLLM_MAX_MODEL_LEN=4096  # Start conservative (default 128k won't fit)
# Can try 8192 if stable, but 4096 is safe

# Throughput (REDUCE for large model)
VLLM_MAX_NUM_SEQS=32  # Lower than 8B models (was 256)
VLLM_MAX_NUM_BATCHED_TOKENS=4096  # Match MAX_MODEL_LEN

# Data type
VLLM_DTYPE=float16  # FP16 as requested (bfloat16 also works)

# Enable chunked prefill (helps with memory)
VLLM_ENABLE_CHUNKED_PREFILL=true
VLLM_MAX_NUM_BATCHED_TOKENS=2048  # Smaller chunks

# Disable speculative decoding (saves VRAM)
# VLLM_USE_V2_BLOCK_MANAGER=false  # Use if OOM issues

# HuggingFace token (REQUIRED for Llama gated models)
HF_TOKEN=hf_your_token_here

# Placement strategy (let vLLM optimize)
CUDA_VISIBLE_DEVICES=0,1  # Both GPUs visible
```

**Test Configuration (Start Here):**
```bash
# Step 1: Validate with small model first
VLLM_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
VLLM_TENSOR_PARALLEL_SIZE=1  # Single GPU test
VLLM_GPU_MEMORY_UTILIZATION=0.85
VLLM_MAX_MODEL_LEN=8192
# Confirm: Should load in ~30 seconds, use ~16GB

# Step 2: Scale to 70B with conservative settings
VLLM_MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct
VLLM_TENSOR_PARALLEL_SIZE=2
VLLM_GPU_MEMORY_UTILIZATION=0.92
VLLM_MAX_MODEL_LEN=4096  # Conservative
VLLM_MAX_NUM_SEQS=32
# Confirm: Should load in 2-4 minutes, use ~50GB

# Step 3: Tune up if stable
VLLM_GPU_MEMORY_UTILIZATION=0.95
VLLM_MAX_MODEL_LEN=6144  # or 8192
VLLM_MAX_NUM_SEQS=64
```

### vLLM-Specific Environment Variables (General)

```bash
# ===================================
# vLLM Configuration (Production)
# ===================================

# Model selection (CRITICAL)
VLLM_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
# Alternatives:
# - mistralai/Mistral-7B-Instruct-v0.3
# - NousResearch/Meta-Llama-3.1-8B-Instruct  (ungated alternative)

# Multi-GPU setup (if you have multiple GPUs)
VLLM_TENSOR_PARALLEL_SIZE=1  # 1 GPU=1, 2 GPUs=2, 4 GPUs=4, etc.
VLLM_PIPELINE_PARALLEL_SIZE=1  # Usually leave at 1

# Memory management (TUNE THIS)
VLLM_GPU_MEMORY_UTILIZATION=0.90  # 0.85-0.95 (higher = more capacity, less headroom)

# Context window (model-dependent)
VLLM_MAX_MODEL_LEN=8192  # Llama-3.1 supports up to 128k, but longer = more VRAM

# Throughput tuning
VLLM_MAX_NUM_SEQS=256  # Concurrent sequences (higher = more throughput)
VLLM_MAX_NUM_BATCHED_TOKENS=8192  # Tokens per batch

# Data type (affects VRAM and speed)
VLLM_DTYPE=auto  # auto, float16, bfloat16
# bfloat16 is faster on Ampere+ GPUs (RTX 30xx, A-series)

# Quantization (for VRAM savings)
# VLLM_QUANTIZATION=awq  # awq (4-bit), gptq, squeezellm
# Use quantized models from: TheBloke on HuggingFace

# KV cache dtype (experimental, saves VRAM)
# VLLM_KV_CACHE_DTYPE=fp8  # fp8, auto

# HuggingFace token (for gated models)
HF_TOKEN=hf_your_token_here  # Get from: https://huggingface.co/settings/tokens

# Download settings
VLLM_DOWNLOAD_DIR=/root/.cache/huggingface  # Match docker volume

# Disable usage stats (optional, for privacy)
VLLM_NO_USAGE_STATS=1

# Logging
VLLM_LOGGING_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
```

### Performance Tuning for logos

**Step 1: Check GPU specs**
```bash
# On logos server
nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv
```

**Step 2: Calculate optimal settings**
```python
# For Llama-3.1-8B-Instruct (FP16)
# Model size: ~16GB
# Available VRAM: [check nvidia-smi]
# Example: 24GB GPU

GPU_MEMORY_UTILIZATION = 0.90  # Use 21.6GB (24 * 0.9)
# Leaves 2.4GB for CUDA overhead

MAX_MODEL_LEN = 8192  # Safe starting point
# Longer contexts = more VRAM needed
# 128k context would need ~40GB VRAM

MAX_NUM_SEQS = 256  # High throughput
# Adjust down if you hit OOM errors
```

**Step 3: Test configuration**
```bash
# Start vLLM with test config
docker compose --profile vllm up -d

# Monitor VRAM usage
watch -n 1 nvidia-smi

# Test inference
curl http://localhost:8001/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "prompt": "What is 2+2?",
    "max_tokens": 50
  }'
```

### Common vLLM Issues & Solutions

**Issue 1: CUDA Out of Memory (OOM)**
```
OutOfMemoryError: CUDA out of memory
```
**Solutions:**
- ‚úÖ Reduce `GPU_MEMORY_UTILIZATION` (try 0.85 instead of 0.90)
- ‚úÖ Reduce `MAX_MODEL_LEN` (try 4096 instead of 8192)
- ‚úÖ Reduce `MAX_NUM_SEQS` (try 128 instead of 256)
- ‚úÖ Use quantized model (AWQ 4-bit saves 50-75% VRAM)
- ‚úÖ Use smaller model (Mistral-7B instead of Llama-8B)

**Issue 2: Model download fails**
```
HTTPError: 401 Client Error: Unauthorized
```
**Solution:**
- Set `HF_TOKEN` environment variable
- Get token from: https://huggingface.co/settings/tokens
- Accept Llama license: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct

**Issue 3: Slow first request**
```
First request takes 30+ seconds
```
**Expected behavior:**
- Model loading takes time (30-120s for 8B, 2-4 min for 70B)
- Subsequent requests are fast (<100ms)
- Use health check with long `start_period` (see docker-compose above)

**Issue 4: Container crashes on startup**
```
vllm container exits immediately
```
**Check:**
```bash
# View logs
docker compose logs vllm

# Common causes:
# - Incompatible GPU (needs Compute Capability 7.0+)
# - NVIDIA drivers not installed
# - Not enough VRAM
```

**Issue 5: Multi-GPU tensor parallelism not working**
```
RuntimeError: Expected all tensors to be on the same device
```
**Solutions:**
- ‚úÖ Ensure `CUDA_VISIBLE_DEVICES=0,1` is set
- ‚úÖ Check both GPUs visible: `nvidia-smi`
- ‚úÖ Verify `TENSOR_PARALLEL_SIZE` matches GPU count
- ‚úÖ Ensure GPUs have similar compute capability (5090 + 4090 is fine)

**Issue 6: Llama 3.1 70B OOM on 5090 + 4090 (56GB)**
```
torch.cuda.OutOfMemoryError: CUDA out of memory
```
**Solutions (in order):**
1. ‚úÖ Reduce `GPU_MEMORY_UTILIZATION` from 0.95 ‚Üí 0.92 ‚Üí 0.90
2. ‚úÖ Reduce `MAX_MODEL_LEN` from 4096 ‚Üí 2048 (shorter context)
3. ‚úÖ Reduce `MAX_NUM_SEQS` from 32 ‚Üí 16 (lower concurrency)
4. ‚úÖ Enable `ENABLE_CHUNKED_PREFILL=true`
5. ‚ö†Ô∏è Use AWQ 4-bit quantization (~40GB, some quality loss)
6. ‚ö†Ô∏è Fall back to smaller model (Llama-3.1-8B or Mixtral-8x7B)

**Issue 7: Slow inference with 70B**
```
Time to first token: 5+ seconds
```
**Expected:**
- First token latency: 1-3 seconds (depends on prompt length)
- Tokens per second: 15-30 tok/s with TP=2 on 5090+4090
- If slower: Check `nvidia-smi` for GPU utilization (should be 90%+)

### üéØ Llama 3.1 70B Deployment Checklist (5090 + 4090)

**Pre-flight checks:**
```bash
# 1. Verify both GPUs visible
nvidia-smi
# Should show: RTX 5090 (GPU 0, 32GB) + RTX 4090 (GPU 1, 24GB)

# 2. Check NVIDIA driver version
nvidia-smi --query-gpu=driver_version --format=csv
# Need: 535+ for optimal vLLM support

# 3. Check available VRAM
nvidia-smi --query-gpu=memory.free --format=csv
# Should show: ~32GB + ~24GB free

# 4. Verify CUDA visible
echo $CUDA_VISIBLE_DEVICES
# Should be empty or "0,1"

# 5. Check HF token
echo $HF_TOKEN
# Should return your token (needed for Llama gated model)
```

**Load test procedure:**
```bash
# Step 1: Start with 8B model (validation)
docker compose --profile vllm up -d
# Edit .env: VLLM_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
# Edit .env: VLLM_TENSOR_PARALLEL_SIZE=1
# Watch: docker compose logs -f vllm
# Wait for: "Avg prompt throughput: X tokens/s"
# Test: curl http://localhost:8001/v1/chat/completions ...
# Expected: Works, ~16GB VRAM used, <1s latency

# Step 2: Scale to 70B (production)
docker compose down
# Edit .env: VLLM_MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct
# Edit .env: VLLM_TENSOR_PARALLEL_SIZE=2
# Edit .env: VLLM_GPU_MEMORY_UTILIZATION=0.92
# Edit .env: VLLM_MAX_MODEL_LEN=4096
docker compose --profile vllm up -d
# Watch: docker compose logs -f vllm (will take 2-4 minutes)
# Monitor VRAM: watch -n 1 nvidia-smi
# Expected: ~26GB on GPU 0, ~24GB on GPU 1

# Step 3: Tune up (if stable)
# Edit .env: VLLM_GPU_MEMORY_UTILIZATION=0.95
# Edit .env: VLLM_MAX_MODEL_LEN=6144 (or 8192)
# Edit .env: VLLM_MAX_NUM_SEQS=64
docker compose restart vllm
```

### vLLM vs Ollama Comparison

| Feature | vLLM | Ollama |
|---------|------|--------|
| **Throughput** | ‚ö°‚ö°‚ö° Highest | ‚ö°‚ö° Good |
| **Setup Complexity** | ‚ö†Ô∏è Moderate | ‚úÖ Easy |
| **API Format** | ‚úÖ OpenAI-compatible | ‚ö†Ô∏è Custom (adapter needed) |
| **Multi-GPU** | ‚úÖ Native tensor parallelism | ‚ùå Single GPU only |
| **Quantization** | ‚úÖ AWQ, GPTQ | ‚úÖ GGUF (different format) |
| **Production Use** | ‚úÖ Battle-tested at scale | ‚ö†Ô∏è Newer, less proven |
| **Memory Efficiency** | ‚úÖ PagedAttention | ‚ö†Ô∏è Standard KV cache |
| **Model Selection** | ‚úÖ HuggingFace (thousands) | ‚úÖ Curated library |

**When to use vLLM:**
- ‚úÖ Production deployment
- ‚úÖ High throughput requirements (>100 req/s)
- ‚úÖ Multi-GPU setup
- ‚úÖ Need OpenAI API compatibility
- ‚úÖ Memory-constrained (PagedAttention helps)

**When to use Ollama:**
- ‚úÖ Development/testing
- ‚úÖ Quick setup needed
- ‚úÖ Single GPU
- ‚úÖ Prefer pre-configured models

### vLLM Monitoring & Observability

**Add Prometheus metrics endpoint:**
```yaml
# docker-compose.yml
vllm:
  environment:
    - VLLM_ENABLE_METRICS=1  # Exposes /metrics endpoint
  ports:
    - "8001:8000"  # API
    - "8002:8080"  # Metrics (optional)
```

**Key metrics to monitor:**
- `vllm:num_requests_running` - Active requests
- `vllm:num_requests_waiting` - Queue depth
- `vllm:gpu_cache_usage_perc` - KV cache utilization
- `vllm:time_to_first_token_seconds` - Latency
- `vllm:time_per_output_token_seconds` - Generation speed

**Quick health check:**
```bash
# Check if vLLM is ready
curl http://localhost:8001/health

# List loaded models
curl http://localhost:8001/v1/models

# Check metrics
curl http://localhost:8001/metrics
```

### üìä Expected Performance (Llama 3.1 70B on 5090 + 4090)

**Hardware baseline:**
- GPU 0: RTX 5090 FE (32GB VRAM, ~73 TFLOPS FP16)
- GPU 1: RTX 4090 (24GB VRAM, ~82 TFLOPS FP16)
- Combined: 56GB VRAM, ~155 TFLOPS FP16

**Llama 3.1 70B FP16 with TP=2 expectations:**

| Metric | Expected Value | Notes |
|--------|---------------|-------|
| **Model load time** | 2-4 minutes | One-time on startup |
| **VRAM usage (GPU 0)** | 26-30GB | ~50-52GB total across both |
| **VRAM usage (GPU 1)** | 22-24GB | Should fit comfortably |
| **Time to first token** | 1-3 seconds | Depends on prompt length |
| **Tokens per second** | 15-30 tok/s | With MAX_MODEL_LEN=4096 |
| **Concurrent requests** | 8-16 | With MAX_NUM_SEQS=32 |
| **Context window** | 4096 tokens | Can try 6144-8192 if stable |
| **Throughput** | 500-1000 tok/s total | With batching |

**Performance tuning tips:**
```bash
# Monitor during inference
watch -n 0.5 nvidia-smi

# Expected during active inference:
# GPU 0: 28GB used, 90%+ utilization, 60-70¬∞C
# GPU 1: 23GB used, 90%+ utilization, 60-70¬∞C

# If underutilized (<50%):
# - Increase MAX_NUM_SEQS (more concurrent requests)
# - Increase batch size

# If overloaded (OOM errors):
# - Decrease GPU_MEMORY_UTILIZATION
# - Decrease MAX_MODEL_LEN
# - Decrease MAX_NUM_SEQS
```

**Benchmark script:**
```bash
# Test single request latency
time curl http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-70B-Instruct",
    "messages": [{"role": "user", "content": "Write a haiku about AI"}],
    "max_tokens": 50
  }'
# Expected: 3-5 seconds total (including network)

# Test throughput (install apache bench)
ab -n 100 -c 10 -p request.json -T application/json \
  http://localhost:8001/v1/chat/completions
# Expected: 10-20 req/s with proper batching
```

**Quality check:**
```bash
# Compare 70B output quality vs OpenAI
curl http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-70B-Instruct",
    "messages": [
      {"role": "user", "content": "Explain quantum entanglement in simple terms"}
    ],
    "max_tokens": 200
  }'

# 70B should provide:
# - Detailed, coherent explanations
# - Better instruction following than 8B
# - Comparable quality to GPT-4 for many tasks
```

**Real-world usage patterns:**
- **Single user (Aletheia chat):** 1-3 tok/s generation, feels instant
- **Batch processing:** 500-1000 tok/s total throughput
- **Cost comparison:** $0/req vs OpenAI GPT-4 $0.03/req (1000 tok)
- **Privacy:** All data stays on logos, no external API calls

**When to scale up/down:**
- ‚úÖ 70B working well ‚Üí Production ready!
- ‚ö†Ô∏è OOM frequently ‚Üí Try AWQ 4-bit (40GB) or fall back to 8B
- ‚ö†Ô∏è Too slow ‚Üí Check GPU utilization, tune batch size
- ‚úÖ Underutilized ‚Üí Increase MAX_NUM_SEQS for more concurrency

---

### Quick health check:
```bash
# Check if vLLM is ready
curl http://localhost:8001/health

# List loaded models
curl http://localhost:8001/v1/models

# Check metrics
curl http://localhost:8001/metrics
```

---

## ÔøΩüîß Implementation Plan

### Phase 1: Provider Abstraction Layer

**Step 1.1: Create Base Provider Interface**

Create `app/services/inference/base.py`:

```python
"""Base inference provider interface.

All inference backends must implement this interface to ensure
compatibility with the Aletheia API.
"""

from abc import ABC, abstractmethod
from typing import Any, Literal


class InferenceProvider(ABC):
    """Base class for all inference providers (OpenAI, Ollama, vLLM, etc)."""

    def __init__(self, config: dict[str, Any]):
        """Initialize provider with configuration.
        
        Args:
            config: Provider-specific configuration dict
        """
        self.config = config
        self._is_healthy = False
        self._last_health_check = None

    @abstractmethod
    def create_chat_completion(
        self,
        messages: list[dict[str, str]],
        model: str,
        temperature: float = 0.7,
        max_tokens: int | None = None,
        stream: bool = False,
        **kwargs
    ) -> dict[str, Any]:
        """Create chat completion (OpenAI-compatible format).
        
        Args:
            messages: Chat messages in OpenAI format
            model: Model identifier
            temperature: Sampling temperature (0.0-2.0)
            max_tokens: Maximum tokens to generate
            stream: Whether to stream response
            **kwargs: Provider-specific parameters
        
        Returns:
            OpenAI-compatible response dict with structure:
            {
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1234567890,
                "model": "model-name",
                "choices": [{
                    "index": 0,
                    "message": {"role": "assistant", "content": "..."},
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": 10,
                    "completion_tokens": 20,
                    "total_tokens": 30
                }
            }
        """
        pass

    @abstractmethod
    def create_embedding(
        self,
        text: str | list[str],
        model: str = "default",
        **kwargs
    ) -> dict[str, Any]:
        """Create text embedding(s) (OpenAI-compatible format).
        
        Args:
            text: Single text or list of texts to embed
            model: Embedding model identifier
            **kwargs: Provider-specific parameters
        
        Returns:
            OpenAI-compatible response dict with structure:
            {
                "object": "list",
                "data": [
                    {"object": "embedding", "index": 0, "embedding": [0.1, 0.2, ...]},
                ],
                "model": "model-name",
                "usage": {"prompt_tokens": 10, "total_tokens": 10}
            }
        """
        pass

    @abstractmethod
    def is_healthy(self) -> bool:
        """Check if provider backend is available and responsive.
        
        Returns:
            True if healthy, False otherwise
        """
        pass

    @abstractmethod
    def get_available_models(self) -> list[str]:
        """List models available from this provider.
        
        Returns:
            List of model identifiers
        """
        pass

    def get_provider_name(self) -> str:
        """Get human-readable provider name."""
        return self.__class__.__name__.replace("Provider", "")

    def get_provider_type(self) -> Literal["api", "local"]:
        """Get provider deployment type."""
        return "api" if self.config.get("api_key") else "local"
```

**Step 1.2: Implement OpenAI Provider**

Create `app/services/inference/openai_provider.py`:

```python
"""OpenAI API provider implementation."""

import time
from openai import OpenAI
from .base import InferenceProvider


class OpenAIProvider(InferenceProvider):
    """OpenAI API inference provider."""

    def __init__(self, config: dict):
        super().__init__(config)
        self.client = OpenAI(
            api_key=config.get("api_key"),
            base_url=config.get("base_url", "https://api.openai.com/v1"),
            timeout=config.get("timeout", 60.0),
        )

    def create_chat_completion(
        self,
        messages: list[dict[str, str]],
        model: str,
        temperature: float = 0.7,
        max_tokens: int | None = None,
        stream: bool = False,
        **kwargs
    ) -> dict:
        """Create chat completion via OpenAI API."""
        response = self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=stream,
            **kwargs
        )
        
        # Convert to dict (OpenAI SDK returns Pydantic models)
        return response.model_dump() if hasattr(response, "model_dump") else response

    def create_embedding(self, text: str | list[str], model: str = "text-embedding-3-small", **kwargs) -> dict:
        """Create embedding via OpenAI API."""
        response = self.client.embeddings.create(
            model=model,
            input=text,
            **kwargs
        )
        return response.model_dump() if hasattr(response, "model_dump") else response

    def is_healthy(self) -> bool:
        """Check OpenAI API availability."""
        try:
            # Simple models list check
            self.client.models.list()
            return True
        except Exception:
            return False

    def get_available_models(self) -> list[str]:
        """List available OpenAI models."""
        try:
            models = self.client.models.list()
            return [m.id for m in models.data if "gpt" in m.id or "embedding" in m.id]
        except Exception:
            return ["gpt-4o", "gpt-4o-mini", "text-embedding-3-small"]  # Fallback
```

**Step 1.3: Implement Ollama Provider**

Create `app/services/inference/ollama_provider.py`:

```python
"""Ollama local inference provider."""

import requests
from .base import InferenceProvider


class OllamaProvider(InferenceProvider):
    """Ollama local inference provider (OpenAI-compatible API)."""

    def __init__(self, config: dict):
        super().__init__(config)
        self.base_url = config.get("base_url", "http://ollama:11434")
        self.timeout = config.get("timeout", 120.0)

    def create_chat_completion(
        self,
        messages: list[dict[str, str]],
        model: str,
        temperature: float = 0.7,
        max_tokens: int | None = None,
        stream: bool = False,
        **kwargs
    ) -> dict:
        """Create chat completion via Ollama API."""
        # Ollama uses slightly different format
        url = f"{self.base_url}/api/chat"
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }
        
        response = requests.post(url, json=payload, timeout=self.timeout)
        response.raise_for_status()
        
        # Convert Ollama format ‚Üí OpenAI format
        ollama_resp = response.json()
        return {
            "id": f"ollama-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": ollama_resp["message"]["content"]
                },
                "finish_reason": "stop" if ollama_resp.get("done") else "length"
            }],
            "usage": {
                "prompt_tokens": ollama_resp.get("prompt_eval_count", 0),
                "completion_tokens": ollama_resp.get("eval_count", 0),
                "total_tokens": ollama_resp.get("prompt_eval_count", 0) + ollama_resp.get("eval_count", 0)
            }
        }

    def create_embedding(self, text: str | list[str], model: str = "nomic-embed-text", **kwargs) -> dict:
        """Create embedding via Ollama API."""
        url = f"{self.base_url}/api/embeddings"
        
        # Handle single text or list
        texts = [text] if isinstance(text, str) else text
        embeddings = []
        
        for idx, t in enumerate(texts):
            payload = {"model": model, "prompt": t}
            response = requests.post(url, json=payload, timeout=self.timeout)
            response.raise_for_status()
            
            embeddings.append({
                "object": "embedding",
                "index": idx,
                "embedding": response.json()["embedding"]
            })
        
        return {
            "object": "list",
            "data": embeddings,
            "model": model,
            "usage": {"prompt_tokens": sum(len(t.split()) for t in texts), "total_tokens": sum(len(t.split()) for t in texts)}
        }

    def is_healthy(self) -> bool:
        """Check Ollama service availability."""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception:
            return False

    def get_available_models(self) -> list[str]:
        """List available Ollama models."""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            response.raise_for_status()
            models = response.json().get("models", [])
            return [m["name"] for m in models]
        except Exception:
            return []
```

**Step 1.4: Implement vLLM Provider (Production)**

Create `app/services/inference/vllm_provider.py`:

```python
"""vLLM provider for high-performance local inference.

vLLM provides OpenAI-compatible API out of the box, making integration simple.
This is the recommended provider for production local inference.
"""

import time
import requests
from .base import InferenceProvider


class vLLMProvider(InferenceProvider):
    """vLLM inference provider (OpenAI-compatible).
    
    vLLM serves models with an OpenAI-compatible API, so we can use
    the same request/response format as OpenAIProvider.
    
    Key features:
    - Native OpenAI API compatibility
    - High throughput (PagedAttention, continuous batching)
    - Multi-GPU support (tensor parallelism)
    - Production-ready
    """

    def __init__(self, config: dict):
        super().__init__(config)
        self.base_url = config.get("base_url", "http://vllm:8000")
        self.timeout = config.get("timeout", 120.0)
        
        # vLLM uses OpenAI-compatible endpoints
        self.chat_url = f"{self.base_url}/v1/chat/completions"
        self.embeddings_url = f"{self.base_url}/v1/embeddings"
        self.models_url = f"{self.base_url}/v1/models"
        self.health_url = f"{self.base_url}/health"

    def create_chat_completion(
        self,
        messages: list[dict[str, str]],
        model: str,
        temperature: float = 0.7,
        max_tokens: int | None = None,
        stream: bool = False,
        **kwargs
    ) -> dict:
        """Create chat completion via vLLM OpenAI-compatible API.
        
        vLLM natively supports OpenAI format, so this is straightforward.
        """
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "stream": stream,
            **kwargs
        }
        
        if max_tokens is not None:
            payload["max_tokens"] = max_tokens
        
        response = requests.post(
            self.chat_url,
            json=payload,
            timeout=self.timeout,
            headers={"Content-Type": "application/json"}
        )
        response.raise_for_status()
        
        # vLLM returns OpenAI-compatible format directly
        return response.json()

    def create_embedding(
        self, 
        text: str | list[str], 
        model: str = "default",
        **kwargs
    ) -> dict:
        """Create embedding via vLLM API.
        
        NOTE: vLLM primarily focuses on text generation. For embeddings,
        consider using a dedicated embedding model or service.
        
        Some vLLM deployments support embeddings via --enable-embeddings flag,
        but this is not the primary use case.
        """
        payload = {
            "model": model,
            "input": text,
            **kwargs
        }
        
        try:
            response = requests.post(
                self.embeddings_url,
                json=payload,
                timeout=self.timeout,
                headers={"Content-Type": "application/json"}
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                raise NotImplementedError(
                    "vLLM embeddings not enabled. Start vLLM with --enable-embeddings "
                    "or use a dedicated embedding provider for this operation."
                )
            raise

    def is_healthy(self) -> bool:
        """Check vLLM service availability via health endpoint."""
        try:
            response = requests.get(self.health_url, timeout=5)
            return response.status_code == 200
        except Exception:
            return False

    def get_available_models(self) -> list[str]:
        """List models available from vLLM server."""
        try:
            response = requests.get(self.models_url, timeout=5)
            response.raise_for_status()
            models_data = response.json()
            return [m["id"] for m in models_data.get("data", [])]
        except Exception:
            # Fallback if models endpoint fails
            return [self.config.get("model_name", "unknown")]
```

**Step 1.5: Create Provider Factory**

Create `app/services/inference/factory.py`:

```python
"""Provider factory for creating inference backends."""

import os
from typing import Literal
from .base import InferenceProvider
from .openai_provider import OpenAIProvider
from .ollama_provider import OllamaProvider
from .vllm_provider import vLLMProvider  # Added!
# from .llamacpp_provider import LlamaCppProvider  # TODO: Phase 3
# from .textgen_provider import TextGenProvider  # TODO: Phase 3


ProviderType = Literal["openai", "ollama", "vllm", "llamacpp", "textgen"]


class ProviderFactory:
    """Factory for creating inference provider instances."""

    @staticmethod
    def create_provider(provider_type: ProviderType | None = None) -> InferenceProvider:
        """Create inference provider based on config.
        
        Args:
            provider_type: Provider to create (defaults to env var)
        
        Returns:
            Configured InferenceProvider instance
        
        Raises:
            ValueError: If provider type unknown or config missing
        """
        if provider_type is None:
            provider_type = os.getenv("INFERENCE_PROVIDER", "openai").lower()

        if provider_type == "openai":
            return OpenAIProvider({
                "api_key": os.getenv("OPENAI_API_KEY"),
                "base_url": os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1"),
                "timeout": float(os.getenv("OPENAI_TIMEOUT", "60")),
            })
        
        elif provider_type == "ollama":
            return OllamaProvider({
                "base_url": os.getenv("LOCAL_INFERENCE_URL", "http://ollama:11434"),
                "timeout": float(os.getenv("LOCAL_INFERENCE_TIMEOUT", "120")),
            })
        
        elif provider_type == "vllm":
            return vLLMProvider({
                "base_url": os.getenv("LOCAL_INFERENCE_URL", "http://vllm:8000"),
                "timeout": float(os.getenv("LOCAL_INFERENCE_TIMEOUT", "120")),
                "model_name": os.getenv("VLLM_MODEL_NAME", "meta-llama/Llama-3.1-8B-Instruct"),
            })
        
        # TODO: Add other providers in Phase 3
        # elif provider_type == "llamacpp":
        #     return LlamaCppProvider({...})
        
        else:
            raise ValueError(f"Unknown provider type: {provider_type}")

    @staticmethod
    def get_supported_providers() -> list[str]:
        """List all supported provider types."""
        return ["openai", "ollama", "vllm"]  # Expand as providers added
```

---

### Phase 2: Inference Router with Fallback

**Step 2.1: Create Router**

Create `app/services/inference/router.py`:

```python
"""Inference router with fallback chain support."""

import os
import logging
from typing import Any
from .base import InferenceProvider
from .factory import ProviderFactory, ProviderType


logger = logging.getLogger(__name__)


class InferenceRouter:
    """Routes inference requests with optional fallback chain."""

    def __init__(self):
        """Initialize router with primary provider and optional fallback chain."""
        self.primary_provider_type = os.getenv("INFERENCE_PROVIDER", "openai")
        self.enable_fallback = os.getenv("ENABLE_FALLBACK", "false").lower() == "true"
        
        # Parse fallback chain (comma-separated)
        fallback_chain_str = os.getenv("FALLBACK_CHAIN", "")
        self.fallback_chain = [
            p.strip() for p in fallback_chain_str.split(",") if p.strip()
        ] if fallback_chain_str else []
        
        # Create primary provider
        self.primary_provider = ProviderFactory.create_provider(self.primary_provider_type)
        
        # Create fallback providers (lazy init)
        self._fallback_providers: dict[str, InferenceProvider] = {}
        
        logger.info(
            f"InferenceRouter initialized: primary={self.primary_provider_type}, "
            f"fallback_enabled={self.enable_fallback}, chain={self.fallback_chain}"
        )

    def _get_fallback_provider(self, provider_type: ProviderType) -> InferenceProvider:
        """Get or create fallback provider (lazy initialization)."""
        if provider_type not in self._fallback_providers:
            self._fallback_providers[provider_type] = ProviderFactory.create_provider(provider_type)
        return self._fallback_providers[provider_type]

    def create_chat_completion(
        self,
        messages: list[dict[str, str]],
        model: str,
        **kwargs
    ) -> dict[str, Any]:
        """Create chat completion with fallback support.
        
        Tries primary provider first, then falls back through chain on failure.
        """
        # Try primary provider
        try:
            response = self.primary_provider.create_chat_completion(
                messages=messages,
                model=model,
                **kwargs
            )
            logger.debug(f"Chat completion succeeded with primary provider: {self.primary_provider_type}")
            return response
        except Exception as e:
            logger.warning(f"Primary provider {self.primary_provider_type} failed: {e}")
            
            if not self.enable_fallback:
                raise

        # Try fallback chain
        for fallback_type in self.fallback_chain:
            try:
                provider = self._get_fallback_provider(fallback_type)
                response = provider.create_chat_completion(
                    messages=messages,
                    model=model,
                    **kwargs
                )
                logger.info(f"Chat completion succeeded with fallback provider: {fallback_type}")
                return response
            except Exception as e:
                logger.warning(f"Fallback provider {fallback_type} failed: {e}")
                continue
        
        # All providers failed
        raise RuntimeError(
            f"All inference providers failed. Primary: {self.primary_provider_type}, "
            f"Fallbacks: {self.fallback_chain}"
        )

    def create_embedding(
        self,
        text: str | list[str],
        model: str = "default",
        **kwargs
    ) -> dict[str, Any]:
        """Create embedding with fallback support."""
        # Try primary provider
        try:
            response = self.primary_provider.create_embedding(
                text=text,
                model=model,
                **kwargs
            )
            logger.debug(f"Embedding succeeded with primary provider: {self.primary_provider_type}")
            return response
        except Exception as e:
            logger.warning(f"Primary provider {self.primary_provider_type} failed: {e}")
            
            if not self.enable_fallback:
                raise

        # Try fallback chain (same logic as chat completion)
        for fallback_type in self.fallback_chain:
            try:
                provider = self._get_fallback_provider(fallback_type)
                response = provider.create_embedding(text=text, model=model, **kwargs)
                logger.info(f"Embedding succeeded with fallback provider: {fallback_type}")
                return response
            except Exception as e:
                logger.warning(f"Fallback provider {fallback_type} failed: {e}")
                continue
        
        raise RuntimeError(
            f"All inference providers failed for embedding. Primary: {self.primary_provider_type}, "
            f"Fallbacks: {self.fallback_chain}"
        )

    def get_health_status(self) -> dict[str, Any]:
        """Get health status of all configured providers."""
        status = {
            "primary": {
                "type": self.primary_provider_type,
                "healthy": self.primary_provider.is_healthy(),
                "models": self.primary_provider.get_available_models() if self.primary_provider.is_healthy() else []
            },
            "fallbacks": []
        }
        
        if self.enable_fallback:
            for fallback_type in self.fallback_chain:
                try:
                    provider = self._get_fallback_provider(fallback_type)
                    status["fallbacks"].append({
                        "type": fallback_type,
                        "healthy": provider.is_healthy(),
                        "models": provider.get_available_models() if provider.is_healthy() else []
                    })
                except Exception as e:
                    status["fallbacks"].append({
                        "type": fallback_type,
                        "healthy": False,
                        "error": str(e)
                    })
        
        return status
```

---

### Phase 3: Integration with Existing Codebase

**Step 3.1: Update OpenAI Service**

Modify `app/services/openai_service.py`:

```python
"""OpenAI service wrapper - now provider-agnostic."""

from app.services.inference.router import InferenceRouter


class OpenAIService:
    """Service for AI operations (chat, embeddings).
    
    NOTE: Name kept for backwards compatibility, but now routes
    to configured inference provider (OpenAI, Ollama, vLLM, etc).
    """

    def __init__(self):
        """Initialize with inference router."""
        self.router = InferenceRouter()

    def create_chat_completion(self, messages: list[dict], model: str, **kwargs) -> dict:
        """Create chat completion via configured provider."""
        return self.router.create_chat_completion(messages=messages, model=model, **kwargs)

    def create_embedding(self, text: str | list[str], model: str = "default", **kwargs) -> dict:
        """Create embedding via configured provider."""
        return self.router.create_embedding(text=text, model=model, **kwargs)

    def get_health_status(self) -> dict:
        """Get health status of inference providers."""
        return self.router.get_health_status()
```

**Step 3.2: Add Health Check Endpoint**

Add to `app/api/routes.py`:

```python
@router.get("/v1/health/inference")
async def inference_health():
    """Check health of inference providers."""
    service = OpenAIService()
    return service.get_health_status()
```

**Step 3.3: Update Environment Configuration**

Add to `.env.example`:

```bash
# ===================================
# Inference Provider Configuration
# ===================================

# Primary inference provider (required)
# Options: openai, ollama, vllm, llamacpp, textgen
INFERENCE_PROVIDER=openai

# OpenAI API Configuration (if INFERENCE_PROVIDER=openai)
OPENAI_API_KEY=sk-your-key-here
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_TIMEOUT=60

# Local Inference Configuration (if INFERENCE_PROVIDER=ollama|vllm|etc)
LOCAL_INFERENCE_URL=http://ollama:11434
LOCAL_MODEL_NAME=llama3.1:8b
LOCAL_INFERENCE_TIMEOUT=120

# Fallback Configuration
ENABLE_FALLBACK=false
FALLBACK_CHAIN=ollama,openai  # Comma-separated, tried in order

# Health Check Configuration
HEALTH_CHECK_INTERVAL=60
HEALTH_CHECK_TIMEOUT=5
```

---

## üìã Deployment Scenarios

### Scenario 1: OpenAI API Only (Current)

**.env:**
```bash
INFERENCE_PROVIDER=openai
OPENAI_API_KEY=sk-...
```

**Docker:**
```bash
docker compose up aletheia-api postgres
```

**Use case:** Production with OpenAI API, minimal infrastructure

---

### Scenario 2: vLLM Local Inference (RECOMMENDED FOR PRODUCTION)

**.env:**
```bash
INFERENCE_PROVIDER=vllm
LOCAL_INFERENCE_URL=http://vllm:8000

# vLLM-specific config
VLLM_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_GPU_MEMORY_UTILIZATION=0.90
VLLM_MAX_MODEL_LEN=8192
VLLM_MAX_NUM_SEQS=256
HF_TOKEN=hf_your_token_here
```

**Docker:**
```bash
# Check GPU availability
nvidia-smi

# Start vLLM service
docker compose --profile vllm up -d

# Wait for model to load (check logs)
docker compose logs -f vllm
# Look for: "Avg prompt throughput: X tokens/s"

# Test inference
curl http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

**Use case:** Production on-prem, high throughput, low latency, data privacy

---

### Scenario 3: Ollama Local Inference (Development)

**.env:**
```bash
INFERENCE_PROVIDER=ollama
LOCAL_INFERENCE_URL=http://ollama:11434
LOCAL_MODEL_NAME=llama3.1:8b
```

**Docker:**
```bash
# Pull model first
docker compose run ollama ollama pull llama3.1:8b

# Start services
docker compose --profile ollama up
```

**Use case:** Development, quick setup, single GPU

---

### Scenario 4: Hybrid with Fallback (High Availability)

**.env:**
```bash
# Primary: Local vLLM (fast, private)
INFERENCE_PROVIDER=vllm
LOCAL_INFERENCE_URL=http://vllm:8000

# Fallback: OpenAI API (if vLLM down)
ENABLE_FALLBACK=true
FALLBACK_CHAIN=openai
OPENAI_API_KEY=sk-...

# vLLM config
VLLM_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
VLLM_GPU_MEMORY_UTILIZATION=0.90
HF_TOKEN=hf_...
```

**Docker:**
```bash
docker compose --profile vllm up
```

**Use case:** High availability, cost optimization, continuous operation

**Benefits:**
- vLLM handles 99% of traffic (fast, private, no cost)
- OpenAI API catches overflow or vLLM downtime
- Best of both worlds

---

### Scenario 5: Development (Local Only)

**.env:**
```bash
INFERENCE_PROVIDER=ollama
LOCAL_INFERENCE_URL=http://ollama:11434
LOCAL_MODEL_NAME=llama3.1:8b
ENABLE_FALLBACK=false
```

**Docker:**
```bash
docker compose --profile ollama up
```

**Use case:** Development without API costs, deterministic testing

---

## üß™ Testing Strategy

### Unit Tests

```python
# tests/test_providers.py

def test_openai_provider_chat():
    """Test OpenAI provider chat completion."""
    provider = OpenAIProvider({"api_key": "test-key"})
    # Mock OpenAI client...
    response = provider.create_chat_completion(
        messages=[{"role": "user", "content": "Hello"}],
        model="gpt-4o"
    )
    assert response["object"] == "chat.completion"

def test_ollama_provider_chat():
    """Test Ollama provider chat completion."""
    provider = OllamaProvider({"base_url": "http://ollama:11434"})
    # Mock requests...
    response = provider.create_chat_completion(
        messages=[{"role": "user", "content": "Hello"}],
        model="llama3.1:8b"
    )
    assert response["object"] == "chat.completion"

def test_router_fallback():
    """Test fallback chain logic."""
    # Mock primary provider failure, fallback success
    router = InferenceRouter()
    # ...
```

### Integration Tests

```python
# tests/test_inference_integration.py

@pytest.mark.integration
def test_chat_with_ollama():
    """Test end-to-end chat with Ollama backend."""
    # Requires Ollama service running
    service = OpenAIService()
    response = service.create_chat_completion(
        messages=[{"role": "user", "content": "What is 2+2?"}],
        model="llama3.1:8b"
    )
    assert "4" in response["choices"][0]["message"]["content"].lower()
```

---

## üìä Comparison Matrix

| Feature | OpenAI API | Ollama | vLLM | llama.cpp | TextGen |
|---------|-----------|--------|------|-----------|---------|
| **Setup Complexity** | ‚≠ê Easy | ‚≠ê‚≠ê Moderate | ‚≠ê‚≠ê‚≠ê Complex | ‚≠ê‚≠ê Moderate | ‚≠ê‚≠ê‚≠ê Complex |
| **Cost** | üí∞üí∞üí∞ Pay-per-token | üí∞ Hardware only | üí∞ Hardware only | üí∞ Hardware only | üí∞ Hardware only |
| **Performance** | ‚ö°‚ö°‚ö° Fast | ‚ö°‚ö° Good | ‚ö°‚ö°‚ö° Excellent | ‚ö° Moderate | ‚ö°‚ö° Good |
| **Privacy** | ‚ùå Cloud | ‚úÖ Local | ‚úÖ Local | ‚úÖ Local | ‚úÖ Local |
| **GPU Required** | ‚ùå No | ‚úÖ Recommended | ‚úÖ Yes | ‚ùå No (CPU OK) | ‚úÖ Recommended |
| **Model Selection** | ‚úÖ Many | ‚úÖ Many | ‚úÖ Many | ‚úÖ GGUF only | ‚úÖ Many |
| **Streaming** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| **Fine-tuning** | ‚úÖ Via API | ‚úÖ Yes | ‚ùå No | ‚ùå No | ‚úÖ Yes (LoRA) |
| **Production Ready** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | ‚ö†Ô∏è Moderate | ‚ö†Ô∏è Moderate |

**Recommendations:**
- **Production:** OpenAI API (ease) or vLLM (performance + control)
- **Development:** Ollama (easiest local setup)
- **CPU-only:** llama.cpp (no GPU required)
- **Experimentation:** TextGen (feature-rich, easy model switching)
- **Hybrid:** OpenAI primary + Ollama fallback (best of both)

---

## üöÄ Quick Start Examples

### Example 1: Switch to Ollama

```bash
# 1. Update .env
INFERENCE_PROVIDER=ollama
LOCAL_INFERENCE_URL=http://ollama:11434

# 2. Start services
docker compose --profile ollama up -d

# 3. Pull model
docker compose exec ollama ollama pull llama3.1:8b

# 4. Test
curl http://localhost:8000/v1/health/inference
```

### Example 2: Enable Fallback

```bash
# 1. Update .env
INFERENCE_PROVIDER=ollama
ENABLE_FALLBACK=true
FALLBACK_CHAIN=openai
OPENAI_API_KEY=sk-...

# 2. Start services (Ollama primary, OpenAI fallback)
docker compose --profile ollama up -d

# 3. If Ollama down, requests automatically fall back to OpenAI
```

---

## üìù Task Checklist

### Phase 1: Core Abstraction (Week 1)
- [ ] Create `app/services/inference/` directory
- [ ] Implement `base.py` (InferenceProvider interface)
- [ ] Implement `openai_provider.py` (existing API)
- [ ] Implement `ollama_provider.py` (local inference)
- [ ] Implement `factory.py` (provider creation)
- [ ] Add unit tests for providers
- [ ] Update `.env.example` with new vars

### Phase 2: Routing & Fallback (Week 1-2)
- [ ] Implement `router.py` (fallback chain logic)
- [ ] Update `openai_service.py` to use router
- [ ] Add health check endpoint
- [ ] Add integration tests
- [ ] Update documentation

### Phase 3: Docker Integration (Week 2)
- [ ] Add Ollama service to docker-compose.yml
- [ ] Add docker-compose profiles
- [ ] Create model download scripts
- [ ] Test profile-based deployment
- [ ] Update deployment docs

### Phase 4: Additional Providers (Week 3+)
- [ ] Implement vLLM provider
- [ ] Implement llama.cpp provider
- [ ] Implement TextGen provider
- [ ] Add provider-specific docs
- [ ] Performance benchmarking

### Phase 5: Production Hardening (Week 4+)
- [ ] Add retry logic to providers
- [ ] Implement request timeouts
- [ ] Add metrics/monitoring
- [ ] Load testing with local backends
- [ ] Production deployment guide

---

## üéØ Success Criteria

- [ ] Can switch between OpenAI and Ollama via env var only
- [ ] Fallback chain works correctly (primary ‚Üí fallback ‚Üí error)
- [ ] Health check endpoint shows all provider status
- [ ] Docker profiles enable easy backend selection
- [ ] No code changes required to switch providers
- [ ] All existing tests pass with new architecture
- [ ] Performance acceptable with local backends
- [ ] Documentation complete for all scenarios

---

## üìö Additional Resources

**Ollama:**
- Docs: https://ollama.ai/docs
- Models: https://ollama.ai/library

**vLLM:**
- Docs: https://docs.vllm.ai/
- GitHub: https://github.com/vllm-project/vllm

**llama.cpp:**
- GitHub: https://github.com/ggerganov/llama.cpp
- Models: https://huggingface.co/models?search=gguf

**Text Generation WebUI:**
- GitHub: https://github.com/oobabooga/text-generation-webui
- Docs: https://github.com/oobabooga/text-generation-webui/wiki
